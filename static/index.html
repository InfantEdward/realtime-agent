<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Realtime Agent App</title>
  <style>
    body {
      font-family: sans-serif;
      margin: 2rem;
    }
    button {
      margin: 0.5rem;
    }
    #sessionId {
      font-weight: bold;
    }
    #transcript {
      white-space: pre-wrap;
      border: 1px solid #ccc;
      padding: 1rem;
      max-width: 600px;
      min-height: 120px;
    }
    #recordIndicator {
      margin-left: 1rem;
      color: red;
      font-weight: bold;
    }
    button:disabled {
      background-color: #ccc;
      cursor: not-allowed;
    }
  </style>
</head>
<body>
  <h1>Realtime Agent App</h1>

  <!-- Session controls -->
  <div>
    <button id="btnStartSession">Start Session</button>
    <button id="btnStopSession">Stop Session</button>
  </div>
  <p>Session ID: <span id="sessionId">N/A</span></p>

  <!-- Toggle recording -->
  <button id="btnToggleRecording" disabled>Start Recording</button>
  <span id="recordIndicator">Not Recording</span>

  <h3>Transcript:</h3>
  <div id="transcript"></div>

  <script>
    // Session & WebSocket
    let sessionId = null;
    let ws = null;

    // For mic recording
    let isRecording = false;
    let audioContext = null;
    let micStream = null;
    let processor = null;

    // For TTS playback scheduling
    let playbackContext = null;
    let nextChunkTime = 0; // time in the AudioContext timeline when the next chunk should start

    // DOM elements
    const btnStartSession = document.getElementById("btnStartSession");
    const btnStopSession = document.getElementById("btnStopSession");
    const btnToggleRecording = document.getElementById("btnToggleRecording");
    const recordIndicator = document.getElementById("recordIndicator");
    const sessionIdSpan = document.getElementById("sessionId");
    const transcriptDiv = document.getElementById("transcript");

    // ---------------------------------------------------------------
    // 1. Start Session -> POST /start_session
    // ---------------------------------------------------------------
    btnStartSession.addEventListener("click", async () => {
      console.log("Requesting new session...");
      const resp = await fetch("/start_session", { method: "POST" });
      const data = await resp.json();
      sessionId = data.session_id;
      sessionIdSpan.textContent = sessionId;
      console.log("Created session:", sessionId);

      // Open WS to /ws/audio/{session_id}
      if (ws) ws.close();
      ws = new WebSocket(`ws://${location.host}/ws/audio/${sessionId}`);
      ws.onopen = () => {
        console.log("WebSocket connected for session:", sessionId);
        btnToggleRecording.disabled = false;
        initTtsPlayback();
      };

      ws.onmessage = (evt) => {
        const msg = JSON.parse(evt.data);
        console.log("WS message:", msg);

        if (msg.type === "transcript_delta") {
          // Show partial or final transcripts
          transcriptDiv.textContent = msg.text;
        } else if (msg.type === "audio_delta") {
          // TTS chunk: schedule it
          queueTtsChunk(msg.audio);
        } else if (msg.type === "error") {
          console.error("Server error:", msg.message);
        }
      };

      ws.onclose = () => {
        console.log("WebSocket closed");
      };
    });

    // ---------------------------------------------------------------
    // 2. Stop Session -> POST /stop_session
    // ---------------------------------------------------------------
    btnStopSession.addEventListener("click", async () => {
      if (!sessionId) {
        console.warn("No active session to stop.");
        return;
      }
      console.log("Stopping session:", sessionId);
      await fetch(`/stop_session?session_id=${sessionId}`, { method: "POST" });
      sessionIdSpan.textContent = "N/A";
      sessionId = null;
      transcriptDiv.textContent = "";
      btnToggleRecording.disabled = true;

      // Close WS
      if (ws) {
        ws.send(JSON.stringify({ type: "disconnect" }));
        ws.close();
        ws = null;
      }

      // Optionally close TTS playback
      if (playbackContext) {
        playbackContext.close();
        playbackContext = null;
      }
    });

    // ---------------------------------------------------------------
    // 3. Toggle Recording (mic) => sends audio_chunk
    // ---------------------------------------------------------------
    btnToggleRecording.addEventListener("click", async () => {
      if (!ws || !sessionId) {
        console.warn("No active session to record.");
        return;
      }
      if (!isRecording) {
        // START
        isRecording = true;
        btnToggleRecording.textContent = "Stop Recording";
        recordIndicator.textContent = "Recording...";

        audioContext = new AudioContext({ sampleRate: 24000 });
        micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        const input = audioContext.createMediaStreamSource(micStream);

        processor = audioContext.createScriptProcessor(2048, 1, 1);
        processor.onaudioprocess = (evt) => {
          const float32Data = evt.inputBuffer.getChannelData(0);
          const int16Data = float32ToInt16(float32Data);
          const b64 = arrayBufferToBase64(int16Data.buffer);
          ws.send(JSON.stringify({ type: "audio_chunk", audio: b64 }));
        };

        input.connect(processor);
        processor.connect(audioContext.destination);
      } else {
        // STOP
        isRecording = false;
        btnToggleRecording.textContent = "Start Recording";
        recordIndicator.textContent = "Not Recording";

        // Cleanup
        if (processor) {
          processor.disconnect();
          processor.onaudioprocess = null;
          processor = null;
        }
        if (micStream) {
          micStream.getTracks().forEach(t => t.stop());
          micStream = null;
        }
        if (audioContext) {
          await audioContext.close();
          audioContext = null;
        }
      }
    });

    // ---------------------------------------------------------------
    // TTS Playback Scheduling
    // ---------------------------------------------------------------
    function initTtsPlayback() {
      if (playbackContext) {
        playbackContext.close();
      }
      playbackContext = new AudioContext({ sampleRate: 24000 });
      nextChunkTime = 0;
      console.log("Initialized new TTS playback AudioContext");
    }

    function queueTtsChunk(audioB64) {
      if (!playbackContext) {
        initTtsPlayback();
      }
      const raw = atob(audioB64);
      const pcmData = new Uint8Array(raw.length);
      for (let i = 0; i < raw.length; i++) {
        pcmData[i] = raw.charCodeAt(i);
      }
      const int16View = new Int16Array(pcmData.buffer);
      const float32Data = new Float32Array(int16View.length);
      for (let i = 0; i < int16View.length; i++) {
        float32Data[i] = int16View[i] / 32767;
      }
      const audioBuf = playbackContext.createBuffer(1, float32Data.length, 24000);
      audioBuf.copyToChannel(float32Data, 0, 0);

      const source = playbackContext.createBufferSource();
      source.buffer = audioBuf;
      source.connect(playbackContext.destination);

      const now = playbackContext.currentTime;
      const startTime = (nextChunkTime < now) ? now : nextChunkTime;
      source.start(startTime);
      nextChunkTime = startTime + audioBuf.duration;
      console.log(`Queued TTS chunk: length=${audioBuf.duration.toFixed(3)}s, start=${startTime.toFixed(3)}s`);
    }

    function float32ToInt16(float32Array) {
      const int16Array = new Int16Array(float32Array.length);
      for (let i = 0; i < float32Array.length; i++) {
        let s = Math.max(-1, Math.min(1, float32Array[i]));
        s = s < 0 ? s * 0x8000 : s * 0x7FFF;
        int16Array[i] = s;
      }
      return int16Array;
    }

    function arrayBufferToBase64(buffer) {
      const binary = String.fromCharCode(...new Uint8Array(buffer));
      return btoa(binary);
    }
  </script>
</body>
</html>
