<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>Realtime Agent App</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Merriweather:wght@400;700&family=Open+Sans:wght@400;600&display=swap"
      rel="stylesheet"
    />
    <style>
      * {
        box-sizing: border-box;
      }
      body {
        margin: 0;
        padding: 10px;
        background: url("/static/app_background.webp") no-repeat center center fixed;
        background-size: cover;
        color: #ede0d4;
        font-family: "Open Sans", sans-serif;
        line-height: 1.6;
      }
      body::before {
        content: "";
        position: fixed;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        background: rgba(12, 8, 6, 0.7);
        z-index: -1;
      }
      h1 {
        font-family: "Merriweather", serif;
        font-size: 2.5rem;
        text-align: center;
        margin: 1rem 0 1rem;
        color: #f8e1c4;
        text-shadow: 0 3px 6px rgba(0, 0, 0, 0.6);
      }
      .instructions-container {
        max-width: 650px;
        margin: 0 auto 0.7rem;
        padding: 1rem;
        background: rgba(28, 20, 18, 0.85);
        border: 1px solid rgba(255, 255, 255, 0.1);
        border-radius: 8px;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.4);
        text-align: center;
        font-size: 0.9rem;
      }
      .controls-container {
        text-align: center;
      }
      .user-input-container {
        max-width: 900px;
        margin: 0.5rem auto;
        padding: 1rem;
        background: rgba(28, 20, 18, 0.85);
        border: 1px solid rgba(255, 255, 255, 0.1);
        border-radius: 8px;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.4);
        height: 75px;
        display: flex;
        align-items: center;
      }
      .user-input-container input {
        width: 95%;
        padding: 0.5rem;
        border: 1px solid #3e2723;
        border-radius: 4px;
        margin-right: 0.5rem;
        font-size: 1rem;
        background: rgba(28, 20, 18, 0.85);
        color: #f5e1c4;
      }
      button {
        margin: 0.5rem;
        padding: 0.8rem 1.5rem;
        font-weight: 600;
        border: 1px solid #8d6e63;
        border-radius: 4px;
        background: linear-gradient(145deg, #5d4037, #4e342e);
        color: #f5e1c4;
        cursor: pointer;
        transition: transform 0.2s, box-shadow 0.2s, background 0.3s;
        font-family: "Open Sans", sans-serif;
      }
      button:hover:enabled {
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
        background: linear-gradient(145deg, #8d6e63, #6d4b3a);
      }
      button:disabled {
        background: #3e2723;
        border-color: #3e2723;
        cursor: not-allowed;
      }
      #sessionId {
        font-weight: 600;
        color: #f5e1c4;
      }
      .transcript-box {
        white-space: pre-wrap;
        padding: 1rem;
        max-width: 900px;
        min-height: 75px;
        border-radius: 8px;
        background: rgba(28, 20, 18, 0.85);
        border: 1px solid rgba(255, 255, 255, 0.1);
        color: #ede0d4;
        font-size: 1rem;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.4);
        margin: 0.5rem auto;
      }
    </style>
  </head>
  <body>
    <h1>Realtime Agent App</h1>
    <div class="instructions-container">
      <p>
        Welcome! Click "Start Session" to begin. Once the session starts,
        "Start Recording" will be enabled. Click it to capture your audio in real time. Click again to pause recording. When you're finished, click "Stop Session" to end.
        Additionally, you can type text in the input field below and click "Send" (or press Enter) to submit text input.
      </p>
    </div>
    <div class="controls-container">
      <div>
        <button id="btnStartSession">Start Session</button>
        <button id="btnStopSession">Stop Session</button>
      </div>
      <p>Session ID: <span id="sessionId">N/A</span></p>
      <button id="btnToggleRecording" disabled>Start Recording</button>
      <span id="recordIndicator">Not Recording</span>
    </div>
    <div class="user-input-container">
      <input type="text" id="userText" placeholder="Type text here..." />
      <button id="btnSendText">Send</button>
    </div>
    <!-- First transcript box for input_audio_transcript events -->
    <div id="inputTranscript" class="transcript-box"></div>
    <!-- Second transcript box for response_audio_transcript_delta or response_text_delta events -->
    <div id="responseTranscript" class="transcript-box"></div>

    <script>
      let sessionId = null;
      let ws = null;
      let isRecording = false;
      let audioContext = null;
      let micStream = null;
      let processor = null;

      // --- Playback-related variables ---
      let playbackContext = null;
      let nextChunkTime = 0;

      // Track which item_id is currently being played.
      let currentItemId = null;

      // The AudioBufferSourceNode for the chunk currently being played (if any).
      let currentSource = null;

      // The time (in seconds, according to AudioContext) when the current chunk starts playing
      let currentChunkStartTime = 0;

      // Accumulate total milliseconds played for the current item
      let totalPlayedDuration = 0;

      const btnStartSession = document.getElementById("btnStartSession");
      const btnStopSession = document.getElementById("btnStopSession");
      const btnToggleRecording = document.getElementById("btnToggleRecording");
      const recordIndicator = document.getElementById("recordIndicator");
      const sessionIdSpan = document.getElementById("sessionId");
      const userText = document.getElementById("userText");
      const btnSendText = document.getElementById("btnSendText");

      // --- Helper function to stop current audio playback ---
      function stopAudioPlayback() {
        if (currentSource) {
          try {
            currentSource.stop();
          } catch(e) {
            // ignore if already stopped
          }
          currentSource.disconnect();
          currentSource = null;
        }
        if (playbackContext) {
          playbackContext.close();
          playbackContext = null;
        }
        nextChunkTime = 0;
        currentChunkStartTime = 0;
      }

      // --- Computes how many ms have been played in the current chunk so far ---
      function getPlayedDuration() {
        if (!playbackContext || !currentChunkStartTime) return 0;
        const elapsedSec = playbackContext.currentTime - currentChunkStartTime;
        // Clamp negative to 0 just in case
        return elapsedSec > 0 ? Math.floor(elapsedSec * 1000) : 0;
      }

      btnStartSession.addEventListener("click", async () => {
        const resp = await fetch("/start_session", { method: "POST" });
        const data = await resp.json();
        sessionId = data.session_id;
        sessionIdSpan.textContent = sessionId;

        // Re-create WS connection
        if (ws) ws.close();
        ws = new WebSocket(`ws://${location.host}/ws/audio/${sessionId}`);
        ws.onopen = () => {
          btnToggleRecording.disabled = false;
          initTtsPlayback();
        };
        ws.onmessage = (evt) => {
          const msg = JSON.parse(evt.data);
          switch (msg.type) {
            case "input_audio_transcript":
              document.getElementById("inputTranscript").textContent = msg.text;
              break;

            case "response_audio_transcript_delta":
            case "response_text_delta":
              document.getElementById("responseTranscript").textContent = msg.text;
              break;

            case "audio_delta":
              // If a new audio_delta arrives with a new item_id, we 
              // stop the old audio and reset playback for the new item.
              if (currentItemId !== msg.item_id) {
                stopAudioPlayback();
                totalPlayedDuration = 0;
                currentItemId = msg.item_id;
                initTtsPlayback();
              }
              // Queue next chunk for whichever item_id is now current
              queueTtsChunk(msg.audio);
              break;

            case "user_audio_started":
              // Only interrupt if we were actually playing something
              if (currentItemId !== null) {
                totalPlayedDuration += getPlayedDuration();
                if (totalPlayedDuration > 0) {
                  ws.send(JSON.stringify({
                    type: "user_interrupt",
                    duration_ms: totalPlayedDuration,
                    item_id: currentItemId
                  }));
                }
                totalPlayedDuration = 0;
                stopAudioPlayback();
                currentItemId = null;
              }
              break;

            case "error":
              console.error("Server error:", msg.message);
              break;
          }
        };
        ws.onclose = () => {
          console.log("WebSocket closed");
        };
      });

      btnStopSession.addEventListener("click", async () => {
        if (!sessionId) return;
        await fetch(`/stop_session?session_id=${sessionId}`, {
          method: "POST",
        });
        sessionIdSpan.textContent = "N/A";
        sessionId = null;
        // Clear both transcript boxes when session stops
        document.getElementById("inputTranscript").textContent = "";
        document.getElementById("responseTranscript").textContent = "";
        btnToggleRecording.disabled = true;
        if (isRecording) {
          stopRecording();
        }
        if (ws) {
          ws.send(JSON.stringify({ type: "disconnect" }));
          ws.close();
          ws = null;
        }
        stopAudioPlayback();
        currentItemId = null;
      });

      btnToggleRecording.addEventListener("click", async () => {
        if (!ws || !sessionId) return;
        if (!isRecording) {
          startRecording();
        } else {
          stopRecording();
        }
      });

      // If the user clicks Send while audio is playing, send user_interrupt.
      btnSendText.addEventListener("click", () => {
        if (!ws || ws.readyState !== WebSocket.OPEN) return;

        // If we are currently playing TTS (currentItemId != null), that means
        // the user is "interrupting" by typing text. Send "user_interrupt" now.
        if (currentItemId !== null) {
          // Accumulate partial chunk time
          totalPlayedDuration += getPlayedDuration();
          if (totalPlayedDuration > 0) {
            ws.send(JSON.stringify({
              type: "user_interrupt",
              duration_ms: totalPlayedDuration,
              item_id: currentItemId
            }));
          }
          totalPlayedDuration = 0;
          stopAudioPlayback();
          currentItemId = null;
        }

        // Now send the user's text
        const currentText = userText.value;
        ws.send(JSON.stringify({ type: "user_input", text: currentText }));

        // Display the typed text in the "inputTranscript" box
        document.getElementById("inputTranscript").textContent = currentText;
        userText.value = "";
      });

      // If the user presses Enter, mimic the Send button
      userText.addEventListener("keydown", (event) => {
        if (event.key === "Enter") {
          event.preventDefault();
          btnSendText.click();
        }
      });

      async function startRecording() {
        isRecording = true;
        btnToggleRecording.textContent = "Stop Recording";
        recordIndicator.textContent = "Recording...";
        try {
          audioContext = new AudioContext({ sampleRate: 24000 });
          micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          const input = audioContext.createMediaStreamSource(micStream);
          processor = audioContext.createScriptProcessor(2048, 1, 1);
          processor.onaudioprocess = (evt) => {
            const float32Data = evt.inputBuffer.getChannelData(0);
            const int16Data = float32ToInt16(float32Data);
            const b64 = arrayBufferToBase64(int16Data.buffer);
            if (ws && ws.readyState === WebSocket.OPEN) {
              ws.send(JSON.stringify({ type: "audio_chunk", audio: b64 }));
            }
          };
          input.connect(processor);
          processor.connect(audioContext.destination);
        } catch (err) {
          console.error("Error accessing microphone:", err);
        }
      }

      async function stopRecording() {
        isRecording = false;
        btnToggleRecording.textContent = "Start Recording";
        recordIndicator.textContent = "Not Recording";
        if (processor) {
          processor.disconnect();
          processor.onaudioprocess = null;
          processor = null;
        }
        if (micStream) {
          micStream.getTracks().forEach((t) => t.stop());
          micStream = null;
        }
        if (audioContext) {
          await audioContext.close();
          audioContext = null;
        }
      }

      function initTtsPlayback() {
        if (playbackContext) {
          playbackContext.close();
        }
        playbackContext = new AudioContext({ sampleRate: 24000 });
        nextChunkTime = 0;
        currentChunkStartTime = 0;
      }

      function queueTtsChunk(audioB64) {
        if (!playbackContext) {
          initTtsPlayback();
        }
        const raw = atob(audioB64);
        const pcmData = new Uint8Array(raw.length);
        for (let i = 0; i < raw.length; i++) {
          pcmData[i] = raw.charCodeAt(i);
        }
        const int16View = new Int16Array(pcmData.buffer);
        const float32Data = new Float32Array(int16View.length);
        for (let i = 0; i < int16View.length; i++) {
          float32Data[i] = int16View[i] / 32767;
        }
        const audioBuf = playbackContext.createBuffer(1, float32Data.length, 24000);
        audioBuf.copyToChannel(float32Data, 0, 0);

        const source = playbackContext.createBufferSource();
        source.buffer = audioBuf;
        source.connect(playbackContext.destination);

        const now = playbackContext.currentTime;
        const startTime = nextChunkTime < now ? now : nextChunkTime;
        // Record the start time for partial‐chunk calculations:
        currentChunkStartTime = startTime;

        source.start(startTime);
        nextChunkTime = startTime + audioBuf.duration;
        currentSource = source;

        // If a chunk plays fully without interruption, add its duration at the end.
        source.onended = () => {
          // We fully played that chunk, so add its duration
          totalPlayedDuration += Math.floor(audioBuf.duration * 1000);
          // Reset chunk start (no partial chunk in progress anymore)
          currentChunkStartTime = 0;
          currentSource = null;
        };
      }

      function float32ToInt16(float32Array) {
        const int16Array = new Int16Array(float32Array.length);
        for (let i = 0; i < float32Array.length; i++) {
          let s = Math.max(-1, Math.min(1, float32Array[i]));
          s = s < 0 ? s * 0x8000 : s * 0x7fff;
          int16Array[i] = s;
        }
        return int16Array;
      }

      function arrayBufferToBase64(buffer) {
        const binary = String.fromCharCode(...new Uint8Array(buffer));
        return btoa(binary);
      }
    </script>
  </body>
</html>
